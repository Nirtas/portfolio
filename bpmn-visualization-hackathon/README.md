## Локальный запуск проекта

Проект имеет микросервисную архитектуру и разворачивается с помощью Docker Compose.

1.  **Конфигурация окружения:**

    - Все необходимые параметры (доступы к БД, адреса сервисов) представлены в файле `.env` в корне проекта. Корректная конфигурация обязательна для работы системы.

2.  **(Опционально) Установка NVIDIA Container Toolkit:**

    - Если для работы AI/ML компонентов (LLM, STT) требуется использование GPU NVIDIA, необходимо установить **NVIDIA Container Toolkit** на хост-машину для предоставления доступа к GPU из Docker-контейнеров.
    - Инструкции по установке доступны в официальной документации: [NVIDIA Container Toolkit Installation Guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).

3.  **Запуск с помощью Docker Compose:**
    Выполните команду:

    ```bash
    docker-compose up -d --build
    ```

    Эта команда соберет и запустит все сервисы проекта в фоновом режиме.
    Нужно дождаться скачивания модели. По умолчанию будет скачиваться qwen2.5-coder-32b-instruct-q8_0.gguf , который весит около 35 ГБ, поэтому docker-compose up может пройти, но запросы будут валиться. Нужно зайти посмотреть логи. Он отображает надпись, что процесс начался, и потом закончился, отображения процесса загрузки нет.

    ```bash
    docker logs qpsk_local_ollama -f
    ```

    Если написано в логах: --- Запуск скрипта скачивания Python --- , то значит, что модель скачивается

    Просмотр логов STT сервиса (тоже качает модель)

    ```bash
    docker logs qpsk_local_stt -f
    ```

4.  **Доступ к приложению:**

    - Веб-интерфейс:

    ```bash
    http://localhost:8080/chat
    ```

    - Доступ к базе данных PostgreSQL осуществляется через pgAdmin с использованием учетных данных из файла `.env`.

5.  **Остановка:**
    Для остановки всех контейнеров выполните:
    ```bash
    docker-compose down
    ```

# Дополнительный комментарий для изменения моделей.

- Выбрать модель можно настроив HF_MODEL_REPO и HF_MODEL_FILE(берём GGUF файл с HF) сейчас стоит qwen2.5-coder-32b-instruct-q8_0.gguf из [Qwen2.5:coder-32B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF). Ещё нужно будет в ollama/Modelfile поменять имя файла на то, которое поставили в HF_MODEL_FILE. /models надо оставить
- размер Whisper можно настроить с помощью переменной WHISPER_MODEL_SIZE

6.  **Улучшения:**
    Можно сильно увеличить качество работы, если использовать значительно мощнее модели(Deepseek R1 или Gemini 2.5 Pro)

# Генерация BPMN-диаграмм из текстовых и голосовых описаний

Проект предоставляет собой инструмент для автоматизированного преобразования текстовых или голосовых описаний бизнес-процессов в BPMN-диаграммы с последующей верификацией корректности. Система предназначена для упрощения визуализации процессов без необходимости глубокого изучения редакторов диаграмм.

## Функциональные возможности

- **Обработка текстовых и голосовых описаний:** Система принимает на вход описание процесса в виде текста или голоса на русском или английском языке.
- **Преобразование голоса в текст:** Голосовой ввод преобразуется в текстовый формат для последующей обработки.
- **NLP-анализ:** Модуль обработки естественного языка анализирует текст, извлекая ключевые сущности (задачи, события, условия) и логические связи между ними.
- **Построение BPMN-диаграмм:** На основе результатов анализа формируется диаграмма процесса в нотации BPMN 2.0 в виде JSON объекта. Поддерживаются следующие элементы:
  - Задачи (Tasks)
  - События (StartEvent, EndEvent)
  - Шлюзы (Gateways): Эксклюзивный (Exclusive), Параллельный (Parallel)
  - Потоки управления (Sequence Flow, Message Flow)
- **Анализ диаграмм AI-агентами:** После генерации диаграммы AI-агенты выполняют проверку на наличие ошибок и несоответствий:
  - Топологические ошибки: разрывы, недостижимые элементы, тупиковые ветви.
  - Логические ошибки: неопределенные или некорректные связи.
- **Рекомендации и исправления:** По результатам анализа система формирует отчет с обнаруженными проблемами и заново отправляет запрос к LLM на доработку.
- **Финальная визуализация:** Отображение итоговой, проверенной и исправленной BPMN-диаграммы.

## Принцип работы

Процесс работы системы включает следующие этапы:

1.  **Ввод данных:** Пользователь предоставляет текстовое или голосовое описание через интерфейс (сайт).
2.  **Преобразование речи (при необходимости):** Голосовой ввод переводится в текст с использованием STT-сервиса.
3.  **NLP-обработка:** Текст анализируется NLP-моделями для извлечения структуры процесса. Используется Qwen2.5:14B-instruct.
4.  **Генерация диаграммы:** На основе извлеченной структуры создается JSON объект, имеющий структуру BPMN-диаграммы.
5.  **Критический анализ:** AI-агенты проверяют диаграмму на корректность и соответствие стандартам.
6.  **Формирование отчета:** Генерируется отчет об ошибках и предложениях по исправлению.
7.  **Коррекция и финализация:** Отчет об ошибках и сгенерированный JSON
    объект заново анализируются LLM и генерируется исправленный JSON
    объект.
8.  **Парсинг JSON объекта в BPMN-диаграмму:** Финальная версия JSON объекта парсится в BPMN-диаграмму в формате xml с помощью собственноручно написанного парсера.
9.  **Отображение результата:** Пользователю предоставляется финальная версия диаграммы с возможностью ее скачать.

## Используемый технологический стек

- **Языки программирования:** Python, JavaScript
- **Фреймворки:** Flask (бэкенд), React.js (фронтенд)
- **AI / ML:** Qwen2.5-coder-14B-instruct(тестировались на нём), OpenAI Whisper
- **BPMN:** `bpmn-js`
- **База данных:** PostgreSQL
- **Речь в текст (STT):** Whisper
- **Инфраструктура:** Docker, Docker Compose
- **Управление БД:** pgAdmin
